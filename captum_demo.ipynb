{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.6.9 64-bit ('venv')",
   "display_name": "Python 3.6.9 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "5304a30962c356af700bd0863a6d925431d94b81baa5f27d674b1764a2a6e9a7"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## PyData Eindhoven 2020 - Explainable AI Tutorial\n",
    "\n",
    "Captum website: https://captum.ai/\n",
    "\n",
    "Algorithms info: https://captum.ai/docs/algorithms"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# installing required packages\n",
    "! pip install captum\n",
    "\n",
    "# downloading imagenet class index file\n",
    "! wget -P meta/ https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
    "\n",
    "# download test images\n",
    "! mkdir img\n",
    "! wget -P img/ -O img/car.jpg https://www.carscoops.com/wp-content/uploads/2018/03/lamborghini-squadra-corse-huracan-300.jpg\n",
    "! wget -P img/ -O img/dog.jpg https://www.dogbreedslist.info/uploads/allimg/dog-pictures/Leonberger-3.jpg\n",
    "! wget -P img/ -O img/cat.jpg http://4.bp.blogspot.com/-bD3QjwYaRds/T9xVNIEsy0I/AAAAAAAAAFc/1vJcHgvoFDE/s1600/Neighbours_Siamese.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libs\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "%matplotlib inline\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models\n",
    "from torchvision import transforms\n",
    "\n",
    "# attribution methods we would like to explore\n",
    "from captum.attr import Saliency\n",
    "from captum.attr import InputXGradient\n",
    "from captum.attr import GuidedBackprop\n",
    "from captum.attr import GuidedGradCam\n",
    "\n",
    "from captum.attr import Occlusion\n",
    "from captum.attr import FeatureAblation\n",
    "from captum.attr import FeaturePermutation\n",
    "\n",
    "# captum visualization method\n",
    "from captum.attr import visualization as viz"
   ]
  },
  {
   "source": [
    "## Preparations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reading imagenet class index\n",
    "labels_path = 'meta/imagenet_class_index.json'\n",
    "with open(labels_path) as json_data:\n",
    "    idx_to_labels = json.load(json_data)\n",
    "\n",
    "# load the resnet 50 model \n",
    "model = models.resnet50(pretrained=True)\n",
    "model = model.eval()\n",
    "\n",
    "# define transforms\n",
    "transform = transforms.Compose([\n",
    " transforms.Resize(256),\n",
    " transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transform_normalize = transforms.Normalize(\n",
    "     mean=[0.485, 0.456, 0.406],\n",
    "     std=[0.229, 0.224, 0.225]\n",
    " )\n",
    "\n",
    "# open image\n",
    "img = Image.open('img/cat.jpg')\n",
    "\n",
    "# transform image to torch tensor of proper shape\n",
    "transformed_img = transform(img)\n",
    "\n",
    "# forward pass \n",
    "input = transform_normalize(transformed_img)\n",
    "input = input.unsqueeze(0)\n",
    "output = model(input)\n",
    "output = F.softmax(output, dim=1)\n",
    "\n",
    "# getting top 3 predictions \n",
    "top_k = 3\n",
    "prediction_score, pred_label_idx = torch.topk(output, top_k)\n",
    "\n",
    "for ind in range(top_k):\n",
    "    pred_label_ind  = pred_label_idx.squeeze()[ind].item() \n",
    "    predicted_label = idx_to_labels[str(pred_label_ind)][1]\n",
    "    print(f'Top {ind+1} (index {pred_label_ind}):', predicted_label, '(', prediction_score.squeeze()[ind].item(), ')')\n",
    "\n",
    "# assign label index for class you want to attribute\n",
    "pred_label_idx = pred_label_idx.squeeze()[0].item() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vis(attribution: torch.Tensor, image: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Visualize attribution map, initial image and masked image. \n",
    "    \"\"\"\n",
    "\n",
    "    # reshape atribution and image tensors to the proper shape\n",
    "    attribution = np.transpose(attribution.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "    image = np.transpose(image.squeeze().cpu().detach().numpy(), (1,2,0))\n",
    "\n",
    "    # plot results\n",
    "    viz.visualize_image_attr_multiple(attribution, \n",
    "                                      image, \n",
    "                                      methods=[\"original_image\",  \"heat_map\", \"masked_image\"], \n",
    "                                      signs=[\"all\", \"absolute_value\", \"absolute_value\"], \n",
    "                                      fig_size=(15, 10),\n",
    "                                      show_colorbar=True,\n",
    "                                      titles=[\"original image\", \"heat_map\", \"masked_image\"])"
   ]
  },
  {
   "source": [
    "# Backpropagation-based methods\n",
    "\n",
    "## Saliency Maps\n",
    "\n",
    "Saliency is a simple approach for computing input attribution, returning the gradient of the output with respect to the input. This approach can be understood as taking a first-order Taylor expansion of the network at the input, and the gradients are simply the coefficients of each feature in the linear representation of the model. The absolute value of these coefficients can be taken to represent feature importance.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saliency = Saliency(model)\n",
    "attributions_saliency = saliency.attribute(input, target=pred_label_idx)\n",
    "vis(attributions_saliency, transformed_img)"
   ]
  },
  {
   "source": [
    "## Input X Gradient\n",
    "\n",
    "Input X Gradient is an extension of the saliency approach, taking the gradients of the output with respect to the input and multiplying by the input feature values. One intuition for this approach considers a linear model; the gradients are simply the coefficients of each input, and the product of the input with a coefficient corresponds to the total contribution of the feature to the linear model's output."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputXgradient = InputXGradient(model)\n",
    "attributions_inputXgradient = inputXgradient.attribute(input, target=pred_label_idx)\n",
    "vis(attributions_inputXgradient, transformed_img)"
   ]
  },
  {
   "source": [
    "## Guided Backpropagation\n",
    "\n",
    "Guided backpropagation and deconvolution compute the gradient of the target output with respect to the input, but backpropagation of ReLU functions is overridden so that only non-negative gradients are backpropagated. In guided backpropagation, the ReLU function is applied to the input gradients, and in deconvolution, the ReLU function is applied to the output gradients and directly backpropagated. Both approaches were proposed in the context of a convolutional network and are generally used for convolutional networks, although they can be applied generically."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_backprop = GuidedBackprop(model)\n",
    "attributions_guided_backprop = guided_backprop.attribute(input, target=pred_label_idx)\n",
    "vis(attributions_guided_backprop, transformed_img)"
   ]
  },
  {
   "source": [
    "## Guided GradCAM \n",
    "\n",
    "Guided GradCAM computes the element-wise product of guided backpropagation attributions with upsampled (layer) GradCAM attributions. GradCAM attributions are computed with respect to a given layer, and attributions are upsampled to match the input size. This approach is designed for convolutional neural networks. The chosen layer is often the last convolutional layer in the network, but any layer that is spatially aligned with the input can be provided.\n",
    "\n",
    "Guided GradCAM was proposed by the authors of GradCAM as a method to combine the high-resolution nature of Guided Backpropagation with the class-discriminative advantages of GradCAM, which has lower resolution due to upsampling from a convolutional layer.\n",
    "\n",
    "![](http://gradcam.cloudcv.org/static/images/network.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_gradcam = GuidedGradCam(model, model.layer4)\n",
    "attributions_guided_gradcam = guided_gradcam.attribute(input, target=pred_label_idx)\n",
    "vis(attributions_guided_gradcam, transformed_img)"
   ]
  },
  {
   "source": [
    "# Perturbation-Based methods\n",
    "\n",
    "## Occlusion \n",
    "\n",
    "Occlusion is a perturbation based approach to compute attribution, involving replacing each contiguous rectangular region with a given baseline / reference, and computing the difference in output. For features located in multiple regions (hyperrectangles), the corresponding output differences are averaged to compute the attribution for that feature. Occlusion is most useful in cases such as images, where pixels in a contiguous rectangular region are likely to be highly correlated.\n",
    "\n",
    "In the example below, we will try to estimate critical areas for the classifier decision by occluding them. \n",
    "Let's run a sliding window of size 60x60 (sliding_window_shapes parameter) with a stide of 40 along both image dimensions (strides parameter). \n",
    "Our baseline value will be 0 (baselines parameter). "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occlusion = Occlusion(model)\n",
    "\n",
    "attributions_occ = occlusion.attribute(input,\n",
    "                                       strides = (3, 40, 40),\n",
    "                                       target=pred_label_idx,\n",
    "                                       sliding_window_shapes=(3, 60, 60),\n",
    "                                       baselines=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis(attributions_occ, transformed_img)\n"
   ]
  }
 ]
}